{"ast":null,"code":"import _construct from \"/Users/michalmizera/Repos/StreamlitMiami/lib/streamlit_lightweight_charts/frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/construct\";\nimport _toConsumableArray from \"/Users/michalmizera/Repos/StreamlitMiami/lib/streamlit_lightweight_charts/frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\nimport { Data } from '../data';\nimport { Schema } from '../schema';\nimport { Chunked } from '../vector/chunked';\nimport { RecordBatch } from '../recordbatch';\nvar noopBuf = new Uint8Array(0);\nvar nullBufs = function nullBufs(bitmapLength) {\n  return [noopBuf, noopBuf, new Uint8Array(bitmapLength), noopBuf];\n};\n/** @ignore */\nexport function ensureSameLengthData(schema, chunks) {\n  var batchLength = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : chunks.reduce(function (l, c) {\n    return Math.max(l, c.length);\n  }, 0);\n  var data;\n  var field;\n  var i = -1,\n    n = chunks.length;\n  var fields = _toConsumableArray(schema.fields);\n  var batchData = [];\n  var bitmapLength = (batchLength + 63 & ~63) >> 3;\n  while (++i < n) {\n    if ((data = chunks[i]) && data.length === batchLength) {\n      batchData[i] = data;\n    } else {\n      (field = fields[i]).nullable || (fields[i] = fields[i].clone({\n        nullable: true\n      }));\n      batchData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength) : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength));\n    }\n  }\n  return [new Schema(fields), batchLength, batchData];\n}\n/** @ignore */\nexport function distributeColumnsIntoRecordBatches(columns) {\n  return distributeVectorsIntoRecordBatches(new Schema(columns.map(function (_ref) {\n    var field = _ref.field;\n    return field;\n  })), columns);\n}\n/** @ignore */\nexport function distributeVectorsIntoRecordBatches(schema, vecs) {\n  return uniformlyDistributeChunksAcrossRecordBatches(schema, vecs.map(function (v) {\n    return v instanceof Chunked ? v.chunks.map(function (c) {\n      return c.data;\n    }) : [v.data];\n  }));\n}\n/** @ignore */\nfunction uniformlyDistributeChunksAcrossRecordBatches(schema, columns) {\n  var fields = _toConsumableArray(schema.fields);\n  var batchArgs = [];\n  var memo = {\n    numBatches: columns.reduce(function (n, c) {\n      return Math.max(n, c.length);\n    }, 0)\n  };\n  var numBatches = 0,\n    batchLength = 0;\n  var i = -1,\n    numColumns = columns.length;\n  var child,\n    childData = [];\n  while (memo.numBatches-- > 0) {\n    for (batchLength = Number.POSITIVE_INFINITY, i = -1; ++i < numColumns;) {\n      childData[i] = child = columns[i].shift();\n      batchLength = Math.min(batchLength, child ? child.length : batchLength);\n    }\n    if (isFinite(batchLength)) {\n      childData = distributeChildData(fields, batchLength, childData, columns, memo);\n      if (batchLength > 0) {\n        batchArgs[numBatches++] = [batchLength, childData.slice()];\n      }\n    }\n  }\n  return [schema = new Schema(fields, schema.metadata), batchArgs.map(function (xs) {\n    return _construct(RecordBatch, [schema].concat(_toConsumableArray(xs)));\n  })];\n}\n/** @ignore */\nfunction distributeChildData(fields, batchLength, childData, columns, memo) {\n  var data;\n  var field;\n  var length = 0,\n    i = -1,\n    n = columns.length;\n  var bitmapLength = (batchLength + 63 & ~63) >> 3;\n  while (++i < n) {\n    if ((data = childData[i]) && (length = data.length) >= batchLength) {\n      if (length === batchLength) {\n        childData[i] = data;\n      } else {\n        childData[i] = data.slice(0, batchLength);\n        data = data.slice(batchLength, length - batchLength);\n        memo.numBatches = Math.max(memo.numBatches, columns[i].unshift(data));\n      }\n    } else {\n      (field = fields[i]).nullable || (fields[i] = field.clone({\n        nullable: true\n      }));\n      childData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength) : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength));\n    }\n  }\n  return childData;\n}","map":{"version":3,"sources":["util/recordbatch.ts"],"names":[],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAKA,SAAS,IAAI,QAAiB,SAAS;AACvC,SAAS,MAAM,QAAe,WAAW;AACzC,SAAS,OAAO,QAAQ,mBAAmB;AAC3C,SAAS,WAAW,QAAQ,gBAAgB;AAE5C,IAAM,OAAO,GAAG,IAAI,UAAU,CAAC,CAAC,CAAC;AACjC,IAAM,QAAQ,GAAG,SAAX,QAAQ,CAAI,YAAoB;EAAA,OAAe,CACjD,OAAO,EAAE,OAAO,EAAE,IAAI,UAAU,CAAC,YAAY,CAAC,EAAE,OAAO,CAC1C;AAAA;AAEjB;AACA,OAAM,SAAU,oBAAoB,CAChC,MAAiB,EACjB,MAA0B,EACqC;EAAA,IAA/D,WAAW,GAAA,SAAA,CAAA,MAAA,QAAA,SAAA,QAAA,SAAA,GAAA,SAAA,MAAG,MAAM,CAAC,MAAM,CAAC,UAAC,CAAC,EAAE,CAAC;IAAA,OAAK,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,MAAM,CAAC;EAAA,GAAE,CAAC,CAAC;EAE/D,IAAI,IAAsB;EAC1B,IAAI,KAAwB;EAC5B,IAAI,CAAC,GAAG,CAAC,CAAC;IAAE,CAAC,GAAG,MAAM,CAAC,MAAM;EAC7B,IAAM,MAAM,GAAA,kBAAA,CAAO,MAAM,CAAC,MAAM,CAAC;EACjC,IAAM,SAAS,GAAG,EAAwB;EAC1C,IAAM,YAAY,GAAG,CAAE,WAAW,GAAG,EAAE,GAAI,CAAC,EAAE,KAAK,CAAC;EACpD,OAAO,EAAE,CAAC,GAAG,CAAC,EAAE;IACZ,IAAI,CAAC,IAAI,GAAG,MAAM,CAAC,CAAC,CAAC,KAAK,IAAI,CAAC,MAAM,KAAK,WAAW,EAAE;MACnD,SAAS,CAAC,CAAC,CAAC,GAAG,IAAI;KACtB,MAAM;MACH,CAAC,KAAK,GAAG,MAAM,CAAC,CAAC,CAAC,EAAE,QAAQ,KAAK,MAAM,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC;QAAE,QAAQ,EAAE;MAAI,CAAE,CAAsB,CAAC;MACtG,SAAS,CAAC,CAAC,CAAC,GAAG,IAAI,GAAG,IAAI,CAAC,kCAAkC,CAAC,WAAW,CAAC,GACpE,IAAI,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC,EAAE,WAAW,EAAE,WAAW,EAAE,QAAQ,CAAC,YAAY,CAAC,CAAqB;IACtG;EACJ;EACD,OAAO,CAAC,IAAI,MAAM,CAAI,MAAM,CAAC,EAAE,WAAW,EAAE,SAAS,CAA4C;AACrG;AAEA;AACA,OAAM,SAAU,kCAAkC,CAA8C,OAA6B,EAAA;EACzH,OAAO,kCAAkC,CAAI,IAAI,MAAM,CAAI,OAAO,CAAC,GAAG,CAAC,UAAA,IAAA;IAAA,IAAG,KAAK,GAAA,IAAA,CAAL,KAAK;IAAA,OAAO,KAAK;EAAA,EAAC,CAAC,EAAE,OAAO,CAAC;AAC3G;AAEA;AACA,OAAM,SAAU,kCAAkC,CAA8C,MAAiB,EAAE,IAAkD,EAAA;EACjK,OAAO,4CAA4C,CAAI,MAAM,EAAE,IAAI,CAAC,GAAG,CAAC,UAAC,CAAC;IAAA,OAAK,CAAC,YAAY,OAAO,GAAG,CAAC,CAAC,MAAM,CAAC,GAAG,CAAC,UAAC,CAAC;MAAA,OAAK,CAAC,CAAC,IAAI;IAAA,EAAC,GAAG,CAAC,CAAC,CAAC,IAAI,CAAC;EAAA,EAAC,CAAC;AAClJ;AAEA;AACA,SAAS,4CAA4C,CAA8C,MAAiB,EAAE,OAA6B,EAAA;EAE/I,IAAM,MAAM,GAAA,kBAAA,CAAO,MAAM,CAAC,MAAM,CAAC;EACjC,IAAM,SAAS,GAAG,EAAoC;EACtD,IAAM,IAAI,GAAG;IAAE,UAAU,EAAE,OAAO,CAAC,MAAM,CAAC,UAAC,CAAC,EAAE,CAAC;MAAA,OAAK,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,MAAM,CAAC;IAAA,GAAE,CAAC;EAAC,CAAE;EAE/E,IAAI,UAAU,GAAG,CAAC;IAAE,WAAW,GAAG,CAAC;EACnC,IAAI,CAAC,GAAW,CAAC,CAAC;IAAE,UAAU,GAAG,OAAO,CAAC,MAAM;EAC/C,IAAI,KAAuB;IAAE,SAAS,GAAuB,EAAE;EAE/D,OAAO,IAAI,CAAC,UAAU,EAAE,GAAG,CAAC,EAAE;IAE1B,KAAK,WAAW,GAAG,MAAM,CAAC,iBAAiB,EAAE,CAAC,GAAG,CAAC,CAAC,EAAE,EAAE,CAAC,GAAG,UAAU,GAAG;MACpE,SAAS,CAAC,CAAC,CAAC,GAAG,KAAK,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC,KAAK,EAAG;MAC1C,WAAW,GAAG,IAAI,CAAC,GAAG,CAAC,WAAW,EAAE,KAAK,GAAG,KAAK,CAAC,MAAM,GAAG,WAAW,CAAC;IAC1E;IAED,IAAI,QAAQ,CAAC,WAAW,CAAC,EAAE;MACvB,SAAS,GAAG,mBAAmB,CAAC,MAAM,EAAE,WAAW,EAAE,SAAS,EAAE,OAAO,EAAE,IAAI,CAAC;MAC9E,IAAI,WAAW,GAAG,CAAC,EAAE;QACjB,SAAS,CAAC,UAAU,EAAE,CAAC,GAAG,CAAC,WAAW,EAAE,SAAS,CAAC,KAAK,EAAE,CAAC;MAC7D;IACJ;EACJ;EACD,OAAO,CACH,MAAM,GAAG,IAAI,MAAM,CAAI,MAAM,EAAE,MAAM,CAAC,QAAQ,CAAC,EAC/C,SAAS,CAAC,GAAG,CAAC,UAAC,EAAE;IAAA,OAAA,UAAA,CAAS,WAAW,GAAC,MAAM,EAAA,MAAA,CAAA,kBAAA,CAAK,EAAE;EAAA,CAAC,CAAC,CACxD;AACL;AAEA;AACA,SAAS,mBAAmB,CAA8C,MAA2B,EAAE,WAAmB,EAAE,SAA6B,EAAE,OAA6B,EAAE,IAA4B,EAAA;EAClN,IAAI,IAAsB;EAC1B,IAAI,KAAwB;EAC5B,IAAI,MAAM,GAAG,CAAC;IAAE,CAAC,GAAG,CAAC,CAAC;IAAE,CAAC,GAAG,OAAO,CAAC,MAAM;EAC1C,IAAM,YAAY,GAAG,CAAE,WAAW,GAAG,EAAE,GAAI,CAAC,EAAE,KAAK,CAAC;EACpD,OAAO,EAAE,CAAC,GAAG,CAAC,EAAE;IACZ,IAAI,CAAC,IAAI,GAAG,SAAS,CAAC,CAAC,CAAC,KAAM,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,KAAK,WAAY,EAAE;MAClE,IAAI,MAAM,KAAK,WAAW,EAAE;QACxB,SAAS,CAAC,CAAC,CAAC,GAAG,IAAI;OACtB,MAAM;QACH,SAAS,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE,WAAW,CAAC;QACzC,IAAI,GAAG,IAAI,CAAC,KAAK,CAAC,WAAW,EAAE,MAAM,GAAG,WAAW,CAAC;QACpD,IAAI,CAAC,UAAU,GAAG,IAAI,CAAC,GAAG,CAAC,IAAI,CAAC,UAAU,EAAE,OAAO,CAAC,CAAC,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC;MACxE;KACJ,MAAM;MACH,CAAC,KAAK,GAAG,MAAM,CAAC,CAAC,CAAC,EAAE,QAAQ,KAAK,MAAM,CAAC,CAAC,CAAC,GAAG,KAAK,CAAC,KAAK,CAAC;QAAE,QAAQ,EAAE;MAAI,CAAE,CAAsB,CAAC;MAClG,SAAS,CAAC,CAAC,CAAC,GAAG,IAAI,GAAG,IAAI,CAAC,kCAAkC,CAAC,WAAW,CAAC,GACpE,IAAI,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI,EAAE,CAAC,EAAE,WAAW,EAAE,WAAW,EAAE,QAAQ,CAAC,YAAY,CAAC,CAAqB;IACtG;EACJ;EACD,OAAO,SAAS;AACpB","sourcesContent":["// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\nimport { Column } from '../column';\nimport { Vector } from '../vector';\nimport { DataType } from '../type';\nimport { Data, Buffers } from '../data';\nimport { Schema, Field } from '../schema';\nimport { Chunked } from '../vector/chunked';\nimport { RecordBatch } from '../recordbatch';\n\nconst noopBuf = new Uint8Array(0);\nconst nullBufs = (bitmapLength: number) => <unknown> [\n    noopBuf, noopBuf, new Uint8Array(bitmapLength), noopBuf\n] as Buffers<any>;\n\n/** @ignore */\nexport function ensureSameLengthData<T extends { [key: string]: DataType } = any>(\n    schema: Schema<T>,\n    chunks: Data<T[keyof T]>[],\n    batchLength = chunks.reduce((l, c) => Math.max(l, c.length), 0)\n) {\n    let data: Data<T[keyof T]>;\n    let field: Field<T[keyof T]>;\n    let i = -1, n = chunks.length;\n    const fields = [...schema.fields];\n    const batchData = [] as Data<T[keyof T]>[];\n    const bitmapLength = ((batchLength + 63) & ~63) >> 3;\n    while (++i < n) {\n        if ((data = chunks[i]) && data.length === batchLength) {\n            batchData[i] = data;\n        } else {\n            (field = fields[i]).nullable || (fields[i] = fields[i].clone({ nullable: true }) as Field<T[keyof T]>);\n            batchData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength)\n                : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength)) as Data<T[keyof T]>;\n        }\n    }\n    return [new Schema<T>(fields), batchLength, batchData] as [Schema<T>, number, Data<T[keyof T]>[]];\n}\n\n/** @ignore */\nexport function distributeColumnsIntoRecordBatches<T extends { [key: string]: DataType } = any>(columns: Column<T[keyof T]>[]): [Schema<T>, RecordBatch<T>[]] {\n    return distributeVectorsIntoRecordBatches<T>(new Schema<T>(columns.map(({ field }) => field)), columns);\n}\n\n/** @ignore */\nexport function distributeVectorsIntoRecordBatches<T extends { [key: string]: DataType } = any>(schema: Schema<T>, vecs: (Vector<T[keyof T]> | Chunked<T[keyof T]>)[]): [Schema<T>, RecordBatch<T>[]] {\n    return uniformlyDistributeChunksAcrossRecordBatches<T>(schema, vecs.map((v) => v instanceof Chunked ? v.chunks.map((c) => c.data) : [v.data]));\n}\n\n/** @ignore */\nfunction uniformlyDistributeChunksAcrossRecordBatches<T extends { [key: string]: DataType } = any>(schema: Schema<T>, columns: Data<T[keyof T]>[][]): [Schema<T>, RecordBatch<T>[]] {\n\n    const fields = [...schema.fields];\n    const batchArgs = [] as [number, Data<T[keyof T]>[]][];\n    const memo = { numBatches: columns.reduce((n, c) => Math.max(n, c.length), 0) };\n\n    let numBatches = 0, batchLength = 0;\n    let i: number = -1, numColumns = columns.length;\n    let child: Data<T[keyof T]>, childData: Data<T[keyof T]>[] = [];\n\n    while (memo.numBatches-- > 0) {\n\n        for (batchLength = Number.POSITIVE_INFINITY, i = -1; ++i < numColumns;) {\n            childData[i] = child = columns[i].shift()!;\n            batchLength = Math.min(batchLength, child ? child.length : batchLength);\n        }\n\n        if (isFinite(batchLength)) {\n            childData = distributeChildData(fields, batchLength, childData, columns, memo);\n            if (batchLength > 0) {\n                batchArgs[numBatches++] = [batchLength, childData.slice()];\n            }\n        }\n    }\n    return [\n        schema = new Schema<T>(fields, schema.metadata),\n        batchArgs.map((xs) => new RecordBatch(schema, ...xs))\n    ];\n}\n\n/** @ignore */\nfunction distributeChildData<T extends { [key: string]: DataType } = any>(fields: Field<T[keyof T]>[], batchLength: number, childData: Data<T[keyof T]>[], columns: Data<T[keyof T]>[][], memo: { numBatches: number }) {\n    let data: Data<T[keyof T]>;\n    let field: Field<T[keyof T]>;\n    let length = 0, i = -1, n = columns.length;\n    const bitmapLength = ((batchLength + 63) & ~63) >> 3;\n    while (++i < n) {\n        if ((data = childData[i]) && ((length = data.length) >= batchLength)) {\n            if (length === batchLength) {\n                childData[i] = data;\n            } else {\n                childData[i] = data.slice(0, batchLength);\n                data = data.slice(batchLength, length - batchLength);\n                memo.numBatches = Math.max(memo.numBatches, columns[i].unshift(data));\n            }\n        } else {\n            (field = fields[i]).nullable || (fields[i] = field.clone({ nullable: true }) as Field<T[keyof T]>);\n            childData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength)\n                : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength)) as Data<T[keyof T]>;\n        }\n    }\n    return childData;\n}\n"]},"metadata":{},"sourceType":"module"}